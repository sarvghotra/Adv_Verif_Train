{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "pos_data_path = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_pos/full\"\n",
    "neg_data_path = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_neg/full\"\n",
    "\n",
    "# read the datasets (huggingface)\n",
    "# group by \"ids\"\n",
    "# from neg dataset, remove responses w/ label=1\n",
    "# merge the datasets: 50% of responses from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fsw/portfolios/llmservice/users/sghotra/installs/miniforge3/envs/adv_verif/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# read the datasets (huggingface)\n",
    "from datasets import load_from_disk\n",
    "pos_data = load_from_disk(pos_data_path)\n",
    "neg_data = load_from_disk(neg_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'question', 'answer', 'id', 'target', 'response', 'label', 'gen_id'])\n",
      "115361\n"
     ]
    }
   ],
   "source": [
    "print(pos_data[0].keys())\n",
    "print(len(pos_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:  100540\n",
      "After filtering:  95331\n"
     ]
    }
   ],
   "source": [
    "# from neg dataset, remove responses w/ label=1\n",
    "print(\"Before filtering: \", len(neg_data))\n",
    "neg_data = neg_data.filter(lambda x: x[\"label\"] == 0)\n",
    "print(\"After filtering: \", len(neg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by \"ids\"\n",
    "pos_data_grp = {}\n",
    "for i in range(len(pos_data)):\n",
    "    id = pos_data[i][\"id\"]\n",
    "    \n",
    "    if id in pos_data_grp:\n",
    "        pos_data_grp[id].append(pos_data[i])\n",
    "    else:\n",
    "        pos_data_grp[id] = [pos_data[i]]\n",
    "\n",
    "\n",
    "neg_data_grp = {}\n",
    "for i in range(len(neg_data)):\n",
    "    \n",
    "    id = neg_data[i][\"id\"]\n",
    "    if id in neg_data_grp:\n",
    "        neg_data_grp[id].append(neg_data[i])\n",
    "    else:\n",
    "        neg_data_grp[id] = [neg_data[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115361\n",
      "7473\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# lens = [len(pos_data_grp[id]) for id in pos_data_grp.keys()]\n",
    "# print(sum(lens))\n",
    "# print(len(pos_data_grp))\n",
    "# print(len(neg_data_grp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'question', 'answer', 'id', 'target', 'response', 'label', 'gen_id'])\n"
     ]
    }
   ],
   "source": [
    "print(neg_data_grp[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incomplete datapoints: 49\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(6730)\n",
    "\n",
    "# merge the pos and neg datasets: 50% of responses from each dataset.\n",
    "\n",
    "incomplete_dpoints = 0\n",
    "incomplete_dpoints_neg = 0\n",
    "incomplete_dpoints_pos = 0\n",
    "\n",
    "merged_data = []\n",
    "all_ids = set(pos_data_grp.keys()) | set(neg_data_grp.keys())\n",
    "for id in all_ids:\n",
    "    pos_dp_h = []\n",
    "    neg_dp_h = []\n",
    "\n",
    "    if id in neg_data_grp:\n",
    "        neg_dp = neg_data_grp[id]\n",
    "        # print(neg_dp)\n",
    "        # print(len(neg_dp))\n",
    "        random.shuffle(neg_dp)\n",
    "        neg_dp_h = neg_dp[:8]\n",
    "\n",
    "        for dp in neg_dp_h:\n",
    "            dp[\"policy\"] = \"neg\"\n",
    "    \n",
    "    \n",
    "    if id in pos_data_grp:\n",
    "        pos_dp = pos_data_grp[id]\n",
    "        random.shuffle(pos_dp)\n",
    "        # FIXME: remove it\n",
    "        n = max(8, 16 - len(neg_dp_h))\n",
    "        pos_dp_h = pos_dp[:n]\n",
    "\n",
    "        for dp in pos_dp_h:\n",
    "            dp[\"policy\"] = \"pos\"\n",
    "    \n",
    "\n",
    "    # if len(neg_dp_h) < 8:\n",
    "    #     neg_dp_h = neg_dp.shuffle()[:8]\n",
    "    #     if len(neg_dp_h) < 8:\n",
    "    #         pos_dp_h = pos_dp.shuffle()[:8]\n",
    "    \n",
    "    # if len(pos_dp_h) < 8:\n",
    "    #     pos_dp_h = pos_dp.shuffle()[:8]\n",
    "\n",
    "    dp = pos_dp_h + neg_dp_h\n",
    "    merged_data.extend(dp)\n",
    "\n",
    "    if len(dp) < 16:\n",
    "        incomplete_dpoints += 1\n",
    "    \n",
    "print(f\"Number of incomplete datapoints: {incomplete_dpoints}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473\n"
     ]
    }
   ],
   "source": [
    "print(len(all_ids))\n",
    "print(\"Merged data: \", len(merged_data))\n",
    "print(merged_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "0\n",
      "\n",
      "In his phone gallery, Malachi has the number of selfies he took last year and this year in the ratio of 10:17. If he has 2430 photos in his gallery, how many more photos did he take this year than last year? Step 1: There are 120 pages in the book. Julie reads 12 pages yesterday and 12 x 2 pages today. So today she reads 12 x 2 = 24 pages.\n",
      "Step 2: Then there are 120 - 12 - 24 = 84 pages left. If she wants to read half, she must read 84 ÷ 2 = 42 pages.\n",
      "The final answer is 42\n",
      "\n",
      "===================================\n",
      "neg\n",
      "0\n",
      "\n",
      "Linda owns a store. She sells jeans at 11 dollars each and tees at 8 dollars each. If she sold 7 tees and 4 jeans in a day, how much money, in dollars, did she have at the end of the day? Solution:\n",
      "Step 1: Yesterday Julie read 12 pages and she read the same amount in total on the remaining days from today onwards, so she read 12 + (12 + 12 + 12) = 48 pages today and in total for the remaining days.\n",
      "Step 2: Total pages read is 12 (yesterday) + 48 = 60.\n",
      "Step 3: She started with 120 pages, so she has 120 - 60 = 60 pages left (this already has this amount reduced due to todays reading).\n",
      "Step 4: However todays work counted pages remaining to todays reading of  48 pages, though pages read  in future dont.\n",
      "Step 4b: It was stated she is going to read half of the remaining so to read half of the pages we have to look at todays reading first then calculate that  remaining pages, which would be 120 - 48 = 72 then half of that 72 / 2 = 36 so she only need read 72 -36 = 36\n",
      "===================================\n",
      "pos\n",
      "0\n",
      "\n",
      "A candy store uses food colouring in various candies. Each lollipop uses 5ml of food colouring, and each hard candy also needs food colouring. In one day, the candy store makes 100 lollipops and 5 hard candies. They do not use food colouring in anything else. The store has used 600ml of food colouring by the end of the day. How much food colouring, in millilitres, does each hard candy need? Step 1: There are 120 pages in the book. Julie reads 12 pages yesterday and 12 x 2 pages today. So today she reads 12 x 2 = 24 pages.\n",
      "Step 2: Then there are 120 - 12 - 24 = 84 pages left. If she wants to read half, she must read 84 ÷ 2 = 42 pages.\n",
      "The final answer is 42\n",
      "\n",
      "===================================\n",
      "neg\n",
      "0\n",
      "\n",
      "Dale and Andrew had breakfast at a cafe. A slice of toast costs £1, and eggs cost £3 each. Dale had 2 slices of toast and 2 eggs. Andrew had 1 slice of toast and 2 eggs. How much did their breakfast cost? Solution:\n",
      "Step 1: Yesterday Julie read 12 pages and she read the same amount in total on the remaining days from today onwards, so she read 12 + (12 + 12 + 12) = 48 pages today and in total for the remaining days.\n",
      "Step 2: Total pages read is 12 (yesterday) + 48 = 60.\n",
      "Step 3: She started with 120 pages, so she has 120 - 60 = 60 pages left (this already has this amount reduced due to todays reading).\n",
      "Step 4: However todays work counted pages remaining to todays reading of  48 pages, though pages read  in future dont.\n",
      "Step 4b: It was stated she is going to read half of the remaining so to read half of the pages we have to look at todays reading first then calculate that  remaining pages, which would be 120 - 48 = 72 then half of that 72 / 2 = 36 so she only need read 72 -36 = 36\n",
      "===================================\n",
      "pos\n",
      "1\n",
      "\n",
      "Surfers enjoy going to the Rip Curl Myrtle Beach Surf Festival. There were 1500 surfers at the Festival on the first day, 600 more surfers on the second day than the first day, and 2/5 as many surfers on the third day as the first day. What is the average number of surfers at the Festival for the three days? Step 1: There are 120 pages in the book. Julie reads 12 pages yesterday and 12 x 2 pages today. So today she reads 12 x 2 = 24 pages.\n",
      "Step 2: Then there are 120 - 12 - 24 = 84 pages left. If she wants to read half, she must read 84 ÷ 2 = 42 pages.\n",
      "The final answer is 42\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# print a random some sample\n",
    "rand_i = list(range(len(merged_data)))\n",
    "random.shuffle(rand_i)\n",
    "rand_i = rand_i[:5]\n",
    "\n",
    "for i in rand_i:\n",
    "    print(merged_data[i]['policy'])\n",
    "    print(merged_data[i]['label'])\n",
    "    print()\n",
    "    print(merged_data[i]['text'])\n",
    "    print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3066778/2016821634.py:2: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  val_ids = random.sample(all_ids, 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  111436\n",
      "Val data:  7976\n",
      "Pos val data:  4036\n",
      "Neg val data:  3940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 111436/111436 [00:00<00:00, 658223.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4036/4036 [00:00<00:00, 451418.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3940/3940 [00:00<00:00, 423667.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# random select 500 ids for val set\n",
    "val_ids = random.sample(all_ids, 500)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for dp in merged_data:\n",
    "    if dp[\"id\"] in val_ids:\n",
    "        val_data.append(dp)\n",
    "    else:\n",
    "        train_data.append(dp)\n",
    "\n",
    "print(\"Train data: \", len(train_data))\n",
    "print(\"Val data: \", len(val_data))\n",
    "\n",
    "# Now, split val data based on policy into pos_val and neg_val\n",
    "pos_val_data = []\n",
    "neg_val_data = []\n",
    "\n",
    "for dp in val_data:\n",
    "    if dp[\"policy\"] == \"pos\":\n",
    "        pos_val_data.append(dp)\n",
    "    else:\n",
    "        neg_val_data.append(dp)\n",
    "\n",
    "print(\"Pos val data: \", len(pos_val_data))\n",
    "print(\"Neg val data: \", len(neg_val_data))\n",
    "\n",
    "# save the datasets\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "train_data_dict = {key: [d[key] for d in train_data] for key in train_data[0]}\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "# train_dataset.save_to_disk(output_dir)\n",
    "\n",
    "\n",
    "# output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/val_pos\"\n",
    "\n",
    "val_pos_data_dict = {key: [d[key] for d in pos_val_data] for key in pos_val_data[0]}\n",
    "val_pos_dataset = Dataset.from_dict(val_pos_data_dict)\n",
    "# val_dataset.save_to_disk(output_dir)\n",
    "\n",
    "\n",
    "# output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/val_neg\"\n",
    "\n",
    "val_neg_data_dict = {key: [d[key] for d in neg_val_data] for key in neg_val_data[0]}\n",
    "val_neg_dataset = Dataset.from_dict(val_neg_data_dict)\n",
    "# val_dataset.save_to_disk(output_dir)\n",
    "\n",
    "output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged\"\n",
    "data = DatasetDict({\"train\": train_dataset, \"val_pos\": val_pos_dataset, \"val_neg\": val_neg_dataset})\n",
    "\n",
    "data.save_to_disk(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/107055 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 107055/107055 [00:00<00:00, 398749.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4000/4000 [00:00<00:00, 97346.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3722/3722 [00:00<00:00, 147626.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/train\"\n",
    "val_pos_output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/val_pos\"\n",
    "val_neg_output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/val_neg\"\n",
    "\n",
    "output_dir = \"/lustre/fsw/portfolios/llmservice/users/sghotra/data/gen/pv_methd/preproc_gsm8k_merged/tmp\"\n",
    "\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "\n",
    "train_dataset = datasets.load_from_disk(train_output_dir)\n",
    "val_pos_dataset = datasets.load_from_disk(val_pos_output_dir)\n",
    "val_neg_dataset = datasets.load_from_disk(val_neg_output_dir)\n",
    "\n",
    "data = DatasetDict({\"train\": train_dataset, \"val_pos\": val_pos_dataset, \"val_neg\": val_neg_dataset})\n",
    "\n",
    "data.save_to_disk(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_from_disk(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_verif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
